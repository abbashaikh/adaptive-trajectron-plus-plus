{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pathlib\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from trajdata import AgentBatch, AgentType, UnifiedDataset, SceneBatch\n",
    "from trajdata.utils.batch_utils import SceneTimeBatcher\n",
    "from trajdata.data_structures.scene_metadata import Scene as trajdata_Scene\n",
    "from trajdata.data_structures.state import StateArray, StateTensor\n",
    "from trajdata.simulation import SimulationScene, sim_metrics, sim_stats, sim_vis\n",
    "from trajdata.visualization.vis import plot_agent_batch\n",
    "\n",
    "import trajectron.evaluation as evaluation\n",
    "import trajectron.visualization as vis\n",
    "# from trajectron.argument_parser import args\n",
    "from trajectron.model.online.online_trajectron import OnlineTrajectron\n",
    "from trajectron.model.model_registrar import ModelRegistrar\n",
    "from trajectron.environment import Environment, Scene, Node, DoubleHeaderNumpyArray, SceneGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_device = None\n",
    "arg_seed = None\n",
    "\n",
    "if not torch.cuda.is_available() or arg_device == 'cpu':\n",
    "    arg_device = torch.device('cpu')\n",
    "else:\n",
    "    if torch.cuda.device_count() == 1:\n",
    "        # If you have CUDA_VISIBLE_DEVICES set, which you should,\n",
    "        # then this will prevent leftover flag arguments from\n",
    "        # messing with the device allocation.\n",
    "        arg_device = 'cuda:0'\n",
    "\n",
    "    arg_device = torch.device(arg_device)\n",
    "\n",
    "if arg_device is None:\n",
    "    arg_device = 'cpu'\n",
    "\n",
    "if arg_seed is not None:\n",
    "    random.seed(arg_seed)\n",
    "    np.random.seed(arg_seed)\n",
    "    torch.manual_seed(arg_seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(arg_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '/home/abbas/Projects/trajectron/adaptive-trajectron-plus-plus/experiments/pedestrians/kf_models'\n",
    "model_dir = os.path.join(log_dir, 'eth_1mode_base_tpp-10_Nov_2024_21_41_22')\n",
    "\n",
    "# Load hyperparameters from json\n",
    "conf = 'config.json'\n",
    "config_file = os.path.join(model_dir, conf)\n",
    "if not os.path.exists(config_file):\n",
    "    raise ValueError('Config json not found!')\n",
    "with open(config_file, 'r') as conf_json:\n",
    "    hyperparams = json.load(conf_json)\n",
    "\n",
    "output_save_dir = os.path.join(model_dir, 'pred_figs')\n",
    "pathlib.Path(output_save_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for matched scene tags: ['eupeds_eth-test_loo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Scenes from eupeds_eth: 100%|██████████| 1/1 [00:00<00:00, 2837.82it/s]\n",
      "Calculating Agent Data (Serially): 100%|██████████| 1/1 [00:00<00:00, 6978.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 scenes in the scene index.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Agent Data Index (16 CPUs): 100%|██████████| 1/1 [00:00<00:00, 377.02it/s]\n",
      "Structuring Agent Data Index: 100%|██████████| 1/1 [00:00<00:00, 4702.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load evaluation environments and scenes\n",
    "attention_radius = defaultdict(\n",
    "    lambda: 20.0\n",
    ")  # Default range is 20m unless otherwise specified.\n",
    "attention_radius[(AgentType.PEDESTRIAN, AgentType.PEDESTRIAN)] = 10.0\n",
    "attention_radius[(AgentType.PEDESTRIAN, AgentType.VEHICLE)] = 20.0\n",
    "attention_radius[(AgentType.VEHICLE, AgentType.PEDESTRIAN)] = 20.0\n",
    "attention_radius[(AgentType.VEHICLE, AgentType.VEHICLE)] = 30.0\n",
    "\n",
    "dataset = UnifiedDataset(\n",
    "    desired_data=[\"eupeds_eth-test_loo\"],\n",
    "    centric =\"agent\",\n",
    "    history_sec=(hyperparams[\"history_sec\"], hyperparams[\"history_sec\"]),\n",
    "    future_sec=(hyperparams[\"prediction_sec\"], hyperparams[\"prediction_sec\"]),\n",
    "    agent_interaction_distances=attention_radius,\n",
    "    incl_robot_future=hyperparams[\"incl_robot_node\"],\n",
    "    incl_raster_map=hyperparams[\"map_encoding\"],\n",
    "    only_predict=[AgentType.PEDESTRIAN],\n",
    "    no_types=[AgentType.UNKNOWN],\n",
    "    num_workers=hyperparams[\"preprocess_workers\"],\n",
    "    cache_location=hyperparams[\"trajdata_cache_dir\"],\n",
    "    standardize_data=False,\n",
    "    data_dirs={  # Remember to change this to match your filesystem!\n",
    "        \"eupeds_eth\": \"~/Projects/trajectron/datasets/eth_ucy_peds\",\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.envs)\n",
    "# print(dataset.np_obs_type)\n",
    "# print(dataset.np_state_type)\n",
    "# print(dataset.obs_format)\n",
    "# print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_sampler=SceneTimeBatcher(dataset),\n",
    "#     collate_fn=dataset.get_collate_fn(),\n",
    "#     num_workers=4,\n",
    "# )\n",
    "\n",
    "# batch: AgentBatch\n",
    "# for batch in tqdm(dataloader):\n",
    "#     print(batch.scene_ts)\n",
    "\n",
    "# print(batch)\n",
    "# print(batch.agent_name)\n",
    "# print(batch.agent_fut_len)\n",
    "# print(batch.agent_hist_len)\n",
    "# # print(batch.data_idx)\n",
    "# print(batch.scene_ts)\n",
    "# print(batch.curr_agent_state[0])\n",
    "# print(batch.agent_hist[0][-1])\n",
    "# print(batch.agent_fut[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dummy environment with a single scene that contains information about the world.\n",
    "# When using this code, feel free to use whichever scene index or initial timestep you wish.\n",
    "scene_idx = 0\n",
    "\n",
    "sim_env_name = \"eupeds_eth\"\n",
    "scene_name = \"biwi_eth-test_loo\"\n",
    "desired_scene: Scene = dataset.get_scene(scene_idx)\n",
    "\n",
    "# print(sim_scene.dataset.centric)\n",
    "\n",
    "# obs: AgentBatch = sim_scene.reset()\n",
    "# print(obs.scene_ts)\n",
    "\n",
    "# obs = sim_scene.step()\n",
    "# print(obs.scene_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(sim_scene.scene))\n",
    "# print(sim_scene.scene.agents)\n",
    "# print(sim_scene.scene.agents)\n",
    "# agent_ids = [item.name for item in sim_scene.scene.agents]\n",
    "# print(agent_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('position', 'x'), ('position', 'y'), ('velocity', 'x'), ('velocity', 'y'), ('acceleration', 'x'), ('acceleration', 'y'), ('heading', 'sin'), ('heading', 'cos')]\n"
     ]
    }
   ],
   "source": [
    "state = hyperparams[\"state\"]\n",
    "state_len = sum([len(state[\"PEDESTRIAN\"][k]) for k in state[\"PEDESTRIAN\"]])\n",
    "\n",
    "data_header = list()\n",
    "for quantity, values in state[\"PEDESTRIAN\"].items():\n",
    "    for value in values:\n",
    "        data_header.append((quantity, value))\n",
    "\n",
    "print(data_header)\n",
    "# state_len\n",
    "# clipped_nodes = get_clipped_nodes(obs, agent_ids, obs.curr_agent_state, hyperparams)\n",
    "# for node in clipped_nodes:\n",
    "#     len_list = [len(state[node.type.name][k]) for k in state[node.type.name]]\n",
    "#     print(len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_states(t):\n",
    "    # get data from observation of nodes at time 't'\n",
    "    # return array of size [num_agents, state_len]\n",
    "    try:\n",
    "        sim_scene: SimulationScene = SimulationScene(\n",
    "            env_name=sim_env_name,\n",
    "            scene_name=scene_name,\n",
    "            scene=desired_scene,\n",
    "            dataset=dataset,\n",
    "            init_timestep=t,\n",
    "            freeze_agents=True,\n",
    "            )\n",
    "        obs: AgentBatch = sim_scene.reset()\n",
    "        num_agents = len(obs.agent_name)\n",
    "        curr_data = np.zeros((num_agents, state_len))\n",
    "        for idx, agent_state in enumerate(obs.curr_agent_state):\n",
    "            curr_data[idx,:-2] = agent_state[:-1].numpy()\n",
    "            heading = agent_state.heading.item()\n",
    "            curr_data[idx,-2] = np.sin(heading)\n",
    "            curr_data[idx,-1] = np.cos(heading)\n",
    "    except ValueError as e:\n",
    "        if str(e)==f\"Initial timestep {t} contains no agents after filtering. Please choose another initial timestep.\":\n",
    "            obs = None\n",
    "            curr_data = np.array([])\n",
    "        else:\n",
    "            raise\n",
    "        \n",
    "    # curr_data = torch.tensor(curr_data)\n",
    "    # curr_data = StateTensor.from_array(curr_data, \"x,y,xd,yd,xdd,ydd,s,c\")\n",
    "    return obs, curr_data\n",
    "\n",
    "# curr_data = get_current_data(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateTensorXYXdYdXddYddSC([12.8100,  4.6100,  2.7000,  0.7250,  0.1250, -0.2500,\n",
      "                            0.2593,  0.9658])\n",
      "StateTensorXYXdYdXddYddSC([nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "--------------------------------------\n",
      "StateTensorXYXdYdXddYddSC([11.3700,  5.8000, -1.8000,  0.1250,  5.1875,  0.6250,\n",
      "                            0.0693, -0.9976])\n",
      "StateTensorXYXdYdXddYddSC([10.3100,  5.9700, -2.6500,  0.4250, -2.1250,  0.7500,\n",
      "                            0.1584, -0.9874])\n"
     ]
    }
   ],
   "source": [
    "timestep = 2\n",
    "try:\n",
    "    sim_scene: SimulationScene = SimulationScene(\n",
    "                env_name=sim_env_name,\n",
    "                scene_name=scene_name,\n",
    "                scene=desired_scene,\n",
    "                dataset=dataset,\n",
    "                init_timestep=timestep,\n",
    "                freeze_agents=True,\n",
    "                )\n",
    "    obs: AgentBatch = sim_scene.reset()\n",
    "    # nodes = get_current_data(obs)\n",
    "except ValueError as e:\n",
    "    if str(e)==f\"Initial timestep {timestep} contains no agents after filtering. Please choose another initial timestep.\":\n",
    "        nodes = []\n",
    "        print(nodes)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "\n",
    "# print(type(obs.agent_hist_len[0]))\n",
    "# print(obs.agent_hist[0,-1])\n",
    "# print(obs.curr_agent_state[0])\n",
    "print(obs.agent_fut[0,1])\n",
    "print(obs.agent_fut[0,2])\n",
    "print(\"--------------------------------------\")\n",
    "# print(obs.agent_hist[1,-1])\n",
    "# print(obs.curr_agent_state[1])\n",
    "print(obs.agent_fut[1,1])\n",
    "print(obs.agent_fut[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('--------------------------------')\n",
    "# print(obs.scene_ts)\n",
    "# print('--------------------------------')\n",
    "# print(obs.num_neigh)\n",
    "# print('--------------------------------')\n",
    "# print(obs.scene_ids)\n",
    "# print('--------------------------------')\n",
    "# print(obs.robot_fut_len)\n",
    "# print('--------------------------------')\n",
    "# print(obs.agent_types)\n",
    "# print('--------------------------------')\n",
    "# print(obs.agent_type)\n",
    "# print('--------------------------------')\n",
    "# print(obs.agent_hist_len)\n",
    "# print('--------------------------------')\n",
    "# print(f\"current={obs.curr_agent_state[0]}\")\n",
    "# curr_yaw = obs.curr_agent_state[0].heading.item()\n",
    "# world_from_agent = np.array(\n",
    "#     [\n",
    "#         [np.cos(curr_yaw), np.sin(curr_yaw)],\n",
    "#         [-np.sin(curr_yaw), np.cos(curr_yaw)],\n",
    "#     ]\n",
    "# )\n",
    "# next_state = np.zeros(7,)\n",
    "# next_state[:2] = obs.agent_fut[0, 0, :2] @ world_from_agent + obs.curr_agent_state[0, :2]\n",
    "# next_state[2:4] = obs.agent_fut[0, 0, 2:4] @ world_from_agent + obs.curr_agent_state[0, 2:4]\n",
    "# next_state[4:6] = obs.agent_fut[0, 0, 4:6] @ world_from_agent + obs.curr_agent_state[0, 4:6]\n",
    "# yaw_ac = obs.agent_fut[0, 0].heading.item()\n",
    "# next_state[-1] = curr_yaw + yaw_ac\n",
    "# print(next_state)\n",
    "# print('--------------------------------')\n",
    "# print(obs.agent_fut_len)\n",
    "# print('--------------------------------')\n",
    "# print(f\"hist={obs.agent_hist[0]}\")\n",
    "# print(f\"agents={obs.agent_name}\")\n",
    "# print(obs.dt)\n",
    "# print(obs.agent_fut_len)\n",
    "# print(obs.agent_fut_extent)\n",
    "# print(f\"future={obs.agent_fut[0, 0]}\")\n",
    "# fut_heading = np.arctan2 (obs.agent_fut[0, 0, -2], obs.agent_fut[0, 0, -1])\n",
    "# print(fut_heading)\n",
    "# print(obs.history_pad_dir)\n",
    "# print(obs.neigh_hist)\n",
    "\n",
    "# print(hyperparams[\"state\"][\"PEDESTRIAN\"].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clipped_nodes(timesteps, hist=False):\n",
    "    length = len(timesteps)\n",
    "    clipped_nodes: List[Node] = list()\n",
    "    node_hist_len: Dict[Node, torch.Tensor] = dict()\n",
    "    for t in timesteps:\n",
    "        # check if time is valid\n",
    "        if t<0:\n",
    "            continue\n",
    "\n",
    "        obs, curr_data = get_agent_states(t)\n",
    "        # check if nodes exist at current time\n",
    "        if obs is None:\n",
    "            continue\n",
    "        \n",
    "        for node in clipped_nodes:\n",
    "            if node.id not in obs.agent_name:\n",
    "                node.data.data = np.vstack((node.data.data, np.full((1, state_len), np.nan)))\n",
    "            else:\n",
    "                idx = obs.agent_name.index(node.id)\n",
    "                node.data.data = np.vstack((node.data.data, curr_data[idx]))\n",
    "                if hist:\n",
    "                    node_hist_len[node] = obs.agent_hist_len[idx]\n",
    "        \n",
    "        for idx, name in enumerate(obs.agent_name):\n",
    "            if name not in [node.id for node in clipped_nodes]:\n",
    "                node_data = DoubleHeaderNumpyArray(curr_data[idx].reshape(1, state_len), data_header)\n",
    "                node = Node(\n",
    "                        node_type=AgentType(obs.agent_type[idx].item()),\n",
    "                        node_id = name,\n",
    "                        data=node_data\n",
    "                        )\n",
    "                clipped_nodes.append(node)\n",
    "            if hist:\n",
    "                node_hist_len[node] = obs.agent_hist_len[idx]\n",
    "\n",
    "    # add padding      \n",
    "    for node in clipped_nodes:\n",
    "        if node.data.data.shape[0]<length:\n",
    "            len_diff = length - node.data.data.shape[0]\n",
    "            node.data.data = np.vstack((np.full((len_diff, state_len), np.nan), node.data.data))\n",
    "                    \n",
    "    return clipped_nodes, node_hist_len\n",
    "\n",
    "# nodes, hist_len = get_clipped_nodes(list(range(100, 101)), hist=True)\n",
    "# print(nodes)\n",
    "# print(nodes[1].first_timestep)\n",
    "# print(nodes[0].data.data)\n",
    "# print(nodes[2].data.data.shape)\n",
    "# print(nodes[0].data.data.shape)\n",
    "# print(hist_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clipped_input_dict(timestep):\n",
    "    input_dict: Dict[Node] = dict()\n",
    "    existing_nodes, nodes_hist_len = get_clipped_nodes(list(range(timestep, timestep+1)), hist=True)\n",
    "    for node in existing_nodes:\n",
    "        input_dict[node] = node.data.data\n",
    "    return input_dict, nodes_hist_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_online_env(obs : AgentBatch, init_timestep):\n",
    "    # test_scene = env.scenes[scene_idx]\n",
    "\n",
    "    online_scene = Scene(timesteps=init_timestep + 1,\n",
    "                         map=None,\n",
    "                         dt=obs.dt[0])\n",
    "    # online_scene.nodes = test_scene.agents\n",
    "    online_scene.nodes, _ = get_clipped_nodes(\n",
    "        timesteps=np.arange(init_timestep - hyperparams['maximum_history_length'],\n",
    "                            init_timestep + 1))\n",
    "    # online_scene.robot = test_scene.robot\n",
    "    online_scene.calculate_scene_graph(attention_radius=attention_radius,\n",
    "                                       edge_addition_filter=hyperparams['edge_addition_filter'],\n",
    "                                       edge_removal_filter=hyperparams['edge_removal_filter'])\n",
    "\n",
    "    env_standardization = {'PEDESTRIAN': {'position': {'x': {'mean': 0, 'std': 1}, 'y': {'mean': 0, 'std': 1}}, 'velocity': {'x': {'mean': 0, 'std': 2}, 'y': {'mean': 0, 'std': 2}}, 'acceleration': {'x': {'mean': 0, 'std': 1}, 'y': {'mean': 0, 'std': 1}}, 'heading': {'sin': {'mean': 0, 'std': 0.1}, 'cos': {'mean': 0, 'std': 0.1}}}}\n",
    "    node_type_list = [AgentType(node_type.item()).name for node_type in obs.agent_type]\n",
    "    return Environment(\n",
    "            node_type_list=node_type_list,\n",
    "            scenes=[online_scene],\n",
    "            attention_radius=attention_radius,\n",
    "            standardization=env_standardization\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([AgentType.PEDESTRIAN/2, AgentType.PEDESTRIAN/3, AgentType.PEDESTRIAN/6, AgentType.PEDESTRIAN/7, AgentType.PEDESTRIAN/8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You need to have at least acceleration, so you want 2 timesteps of prior data, e.g. [0, 1],\n",
    "# so that you can immediately start incremental inference from the 3rd timestep onwards.\n",
    "init_timestep = 23\n",
    "\n",
    "sim_scene: SimulationScene = SimulationScene(\n",
    "    env_name=sim_env_name,\n",
    "    scene_name=scene_name,\n",
    "    scene=desired_scene,\n",
    "    dataset=dataset,\n",
    "    init_timestep=init_timestep,\n",
    "    freeze_agents=True,\n",
    "    )\n",
    "obs: AgentBatch = sim_scene.reset()\n",
    "\n",
    "hyperparams[\"maximum_history_length\"] = int((hyperparams[\"history_sec\"]/sim_scene.scene.dt) + 1)\n",
    "online_env = create_online_env(obs, init_timestep)\n",
    "\n",
    "model_registrar = ModelRegistrar(model_dir, arg_device)\n",
    "\n",
    "trajectron = OnlineTrajectron(model_registrar,\n",
    "                                hyperparams,\n",
    "                                arg_device)\n",
    "\n",
    "epoch = 50\n",
    "model_path = pathlib.Path(model_dir) / f'model_registrar-{epoch}.pt'\n",
    "checkpoint = torch.load(model_path, map_location=arg_device)\n",
    "trajectron.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "\n",
    "# trajectron.set_environment(online_env, init_timestep)\n",
    "trajectron.env = online_env\n",
    "trajectron.scene_graph = SceneGraph(edge_radius=online_env.attention_radius)\n",
    "trajectron.nodes.clear()\n",
    "trajectron.node_data.clear()\n",
    "trajectron.node_models_dict.clear()\n",
    "\n",
    "# trajectron.device\n",
    "\n",
    "# run trajectron model forward to init_timestep\n",
    "for t in range(init_timestep + 1):\n",
    "    # print(f\"Timestep: {t}\")\n",
    "    new_inputs_dict, nodes_hist_len = get_clipped_input_dict(t)\n",
    "    # print(new_inputs_dict.keys())\n",
    "    trajectron.incremental_forward(\n",
    "        new_inputs_dict=new_inputs_dict,\n",
    "        nodes_hist_len=nodes_hist_len,\n",
    "        maps=None,\n",
    "        run_models=False\n",
    "    )\n",
    "trajectron.node_models_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m new_inputs_dict, nodes_hist_len \u001b[38;5;241m=\u001b[39m get_clipped_input_dict(t)\n\u001b[1;32m      5\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m dists, preds \u001b[38;5;241m=\u001b[39m \u001b[43mtrajectron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincremental_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_inputs_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_inputs_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes_hist_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes_hist_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_horizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m: took \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m s (= \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m Hz) w/ \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m nodes and \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m edges\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (t, end \u001b[38;5;241m-\u001b[39m start,\n\u001b[1;32m     16\u001b[0m                                                                 \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m (end \u001b[38;5;241m-\u001b[39m start), \u001b[38;5;28mlen\u001b[39m(trajectron\u001b[38;5;241m.\u001b[39mnodes),\n\u001b[1;32m     17\u001b[0m                                                                 trajectron\u001b[38;5;241m.\u001b[39mscene_graph\u001b[38;5;241m.\u001b[39mget_num_edges()))\n",
      "File \u001b[0;32m~/Projects/trajectron/adaptive-trajectron-plus-plus/src/trajectron/model/online/online_trajectron.py:281\u001b[0m, in \u001b[0;36mOnlineTrajectron.incremental_forward\u001b[0;34m(self, new_inputs_dict, nodes_hist_len, maps, prediction_horizon, num_samples, robot_present_and_future, z_mode, gmm_mode, full_dist, all_z_sep, run_models)\u001b[0m\n\u001b[1;32m    276\u001b[0m     robot_present_and_future \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m    277\u001b[0m         robot_present_and_future, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    278\u001b[0m     )\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_models_dict:\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_models_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_st\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes_hist_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobot_present_and_future\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaps\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# If num_predicted_timesteps or num_samples == 0 then do not run the decoder at all,\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# just update the encoder LSTMs.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prediction_horizon \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m num_samples \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/trajectron/adaptive-trajectron-plus-plus/src/trajectron/model/online/online_mgcvae.py:484\u001b[0m, in \u001b[0;36mOnlineMultimodalGenerativeCVAE.encoder_forward\u001b[0;34m(self, inputs, inputs_st, inputs_np, nodes_hist_len, robot_present_and_future, maps)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoder_forward\u001b[39m(\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m, inputs, inputs_st, inputs_np, nodes_hist_len, robot_present_and_future\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, maps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    480\u001b[0m ):\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Always predicting with the online model.\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     mode \u001b[38;5;241m=\u001b[39m ModeKeys\u001b[38;5;241m.\u001b[39mPREDICT\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobtain_encoded_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_st\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes_hist_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobot_present_and_future\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaps\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_s_t0 \u001b[38;5;241m=\u001b[39m inputs_st[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode]\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent\u001b[38;5;241m.\u001b[39mp_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_z_x(mode, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[0;32m~/Projects/trajectron/adaptive-trajectron-plus-plus/src/trajectron/model/online/online_mgcvae.py:241\u001b[0m, in \u001b[0;36mOnlineMultimodalGenerativeCVAE.obtain_encoded_tensors\u001b[0;34m(self, mode, inputs, inputs_st, inputs_np, nodes_hist_len, robot_present_and_future, maps)\u001b[0m\n\u001b[1;32m    239\u001b[0m     num_neighbors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_types[edge_type]\n\u001b[1;32m    240\u001b[0m     node_hist_len \u001b[38;5;241m=\u001b[39m nodes_hist_len[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode]\n\u001b[0;32m--> 241\u001b[0m     total_edge_influence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_total_edge_influence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_edges_encoded\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_history_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_hist_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTD \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_history_encoded\u001b[39m\u001b[38;5;124m\"\u001b[39m: node_history_encoded,\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_edge_influence\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_edge_influence,\n\u001b[1;32m    248\u001b[0m }\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m################\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Map Encoding #\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m################\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/trajectron/adaptive-trajectron-plus-plus/src/trajectron/model/mgcvae.py:1048\u001b[0m, in \u001b[0;36mMultimodalGenerativeCVAE.encode_total_edge_influence\u001b[0;34m(self, mode, encoded_edges, num_neighbors, node_history_encoder, node_history_len, batch_size)\u001b[0m\n\u001b[1;32m   1045\u001b[0m with_neighbors \u001b[38;5;241m=\u001b[39m num_neighbors \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1046\u001b[0m combined_edges \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(node_history_encoder)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1048\u001b[0m key_padding_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_neighbors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_neighbors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiagonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1056\u001b[0m combined_edges[:, with_neighbors], attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_modules[\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_type \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/edge_influence_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1058\u001b[0m ](\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1064\u001b[0m )\n\u001b[1;32m   1065\u001b[0m combined_edges \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(\n\u001b[1;32m   1066\u001b[0m     combined_edges\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m   1067\u001b[0m     p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrnn_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout_keep_prob\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1068\u001b[0m     training\u001b[38;5;241m=\u001b[39m(mode \u001b[38;5;241m==\u001b[39m ModeKeys\u001b[38;5;241m.\u001b[39mTRAIN),\n\u001b[1;32m   1069\u001b[0m )\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "for t in range(init_timestep+1, sim_scene.scene.length_timesteps):\n",
    "    print(f\"Timestep: {t}\")\n",
    "    new_xyzh_dict: Dict[str, StateArray] = dict()\n",
    "    new_inputs_dict, nodes_hist_len = get_clipped_input_dict(t)\n",
    "\n",
    "    start = time.time()\n",
    "    dists, preds = trajectron.incremental_forward(\n",
    "        new_inputs_dict=new_inputs_dict,\n",
    "        nodes_hist_len=nodes_hist_len,\n",
    "        maps=None,\n",
    "        prediction_horizon=6,\n",
    "        num_samples=1,\n",
    "        full_dist=True\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(\"t=%d: took %.2f s (= %.2f Hz) w/ %d nodes and %d edges\" % (t, end - start,\n",
    "                                                                    1. / (end - start), len(trajectron.nodes),\n",
    "                                                                    trajectron.scene_graph.get_num_edges()))\n",
    "    \n",
    "    detailed_preds_dict = dict()\n",
    "    for name in obs.agent_name:\n",
    "        if name in preds:\n",
    "            detailed_preds_dict[name] = preds[name]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    vis.visualize_distribution(ax,\n",
    "                            dists)\n",
    "    vis.visualize_prediction(ax,\n",
    "                            {t: preds},\n",
    "                            sim_scene.scene.dt,\n",
    "                            hyperparams['maximum_history_length'],\n",
    "                            hyperparams['prediction_horizon'])\n",
    "    \n",
    "    fig.savefig(os.path.join(output_save_dir, f'pred_{t}.pdf'), dpi=300)\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaptive-trajectron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
