{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pathlib\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from trajdata import AgentBatch, AgentType, UnifiedDataset, SceneBatch\n",
    "from trajdata.utils.batch_utils import SceneTimeBatcher\n",
    "from trajdata.data_structures.scene_metadata import Scene as trajdata_Scene\n",
    "from trajdata.data_structures.state import StateArray, StateTensor\n",
    "from trajdata.simulation import SimulationScene, sim_metrics, sim_stats, sim_vis\n",
    "from trajdata.visualization.vis import plot_agent_batch\n",
    "\n",
    "import trajectron.evaluation as evaluation\n",
    "import trajectron.visualization as vis\n",
    "# from trajectron.argument_parser import args\n",
    "from trajectron.model.online.online_trajectron import OnlineTrajectron\n",
    "from trajectron.model.model_registrar import ModelRegistrar\n",
    "from trajectron.environment import Environment, Scene, Node, DoubleHeaderNumpyArray, SceneGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_device = None\n",
    "arg_seed = None\n",
    "\n",
    "if not torch.cuda.is_available() or arg_device == 'cpu':\n",
    "    arg_device = torch.device('cpu')\n",
    "else:\n",
    "    if torch.cuda.device_count() == 1:\n",
    "        # If you have CUDA_VISIBLE_DEVICES set, which you should,\n",
    "        # then this will prevent leftover flag arguments from\n",
    "        # messing with the device allocation.\n",
    "        arg_device = 'cuda:0'\n",
    "\n",
    "    arg_device = torch.device(arg_device)\n",
    "\n",
    "if arg_device is None:\n",
    "    arg_device = 'cpu'\n",
    "\n",
    "if arg_seed is not None:\n",
    "    random.seed(arg_seed)\n",
    "    np.random.seed(arg_seed)\n",
    "    torch.manual_seed(arg_seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(arg_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '/home/abbas/Projects/trajectron/adaptive-trajectron-plus-plus/experiments/pedestrians/kf_models'\n",
    "model_dir = os.path.join(log_dir, 'eth_1mode_base_tpp-20_Jan_2023_17_38_09')\n",
    "\n",
    "# Load hyperparameters from json\n",
    "conf = 'config.json'\n",
    "config_file = os.path.join(model_dir, conf)\n",
    "if not os.path.exists(config_file):\n",
    "    raise ValueError('Config json not found!')\n",
    "with open(config_file, 'r') as conf_json:\n",
    "    hyperparams = json.load(conf_json)\n",
    "\n",
    "output_save_dir = os.path.join(model_dir, 'pred_figs')\n",
    "pathlib.Path(output_save_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for matched scene tags: ['eupeds_eth-zurich-train', 'eupeds_eth-cyprus-train']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Scenes from eupeds_eth: 100%|██████████| 2/2 [00:00<00:00, 1752.37it/s]\n",
      "Calculating Agent Data (Serially): 100%|██████████| 1/1 [00:00<00:00, 5295.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 scenes in the scene index.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Agent Data Index (8 CPUs): 100%|██████████| 1/1 [00:00<00:00, 122.40it/s]\n",
      "Structuring Agent Data Index: 100%|██████████| 1/1 [00:00<00:00, 582.79it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load evaluation environments and scenes\n",
    "attention_radius = defaultdict(\n",
    "    lambda: 20.0\n",
    ")  # Default range is 20m unless otherwise specified.\n",
    "attention_radius[(AgentType.PEDESTRIAN, AgentType.PEDESTRIAN)] = 10.0\n",
    "attention_radius[(AgentType.PEDESTRIAN, AgentType.VEHICLE)] = 20.0\n",
    "attention_radius[(AgentType.VEHICLE, AgentType.PEDESTRIAN)] = 20.0\n",
    "attention_radius[(AgentType.VEHICLE, AgentType.VEHICLE)] = 30.0\n",
    "\n",
    "dataset = UnifiedDataset(\n",
    "    # desired_data=[\"eupeds_eth-test_loo\"],\n",
    "    desired_data=[\"eupeds_eth-train\"],\n",
    "    centric =\"agent\",\n",
    "    history_sec=(hyperparams[\"history_sec\"], hyperparams[\"history_sec\"]),\n",
    "    future_sec=(hyperparams[\"prediction_sec\"], hyperparams[\"prediction_sec\"]),\n",
    "    agent_interaction_distances=attention_radius,\n",
    "    incl_robot_future=hyperparams[\"incl_robot_node\"],\n",
    "    incl_raster_map=hyperparams[\"map_encoding\"],\n",
    "    only_predict=[AgentType.PEDESTRIAN],\n",
    "    no_types=[AgentType.UNKNOWN],\n",
    "    num_workers=hyperparams[\"preprocess_workers\"],\n",
    "    cache_location=hyperparams[\"trajdata_cache_dir\"],\n",
    "    standardize_data=True,\n",
    "    data_dirs={  # Remember to change this to match your filesystem!\n",
    "        \"eupeds_eth\": \"/home/abbas/Projects/trajectron/datasets/eth_ucy\",\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dummy environment with a single scene that contains information about the world.\n",
    "# When using this code, feel free to use whichever scene index or initial timestep you wish.\n",
    "scene_idx = 0\n",
    "\n",
    "# You need to have at least acceleration, so you want 2 timesteps of prior data, e.g. [0, 1],\n",
    "# so that you can immediately start incremental inference from the 3rd timestep onwards.\n",
    "init_timestep = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the desired scene\n",
    "\n",
    "sim_env_name = \"eupeds_eth\"\n",
    "scene_name = \"biwi_eth-test_loo\"\n",
    "desired_scene: trajdata_Scene = dataset.get_scene(scene_idx)\n",
    "\n",
    "sim_scene: SimulationScene = SimulationScene(\n",
    "            env_name=sim_env_name,\n",
    "            scene_name=scene_name,\n",
    "            scene=desired_scene,\n",
    "            dataset=dataset,\n",
    "            init_timestep=init_timestep,\n",
    "            freeze_agents=True,\n",
    "            )\n",
    "# obs: AgentBatch = sim_scene.reset()\n",
    "\n",
    "# print(obs.agent_name)\n",
    "# print(obs.neigh_hist[2].shape)\n",
    "# # print(obs.neigh_hist_len[2, :obs.num_neigh[2]].unsqueeze(0))\n",
    "# a = obs.neigh_hist_len[2, :obs.num_neigh[2]].unsqueeze(0)\n",
    "# print(torch.max(a))\n",
    "# # print(obs.num_neigh[0].shape)\n",
    "# print(obs.neigh_types[1, :obs.num_neigh[1]].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "\n",
    "# hyperparams[\"pred_state\"]\n",
    "# agent_type = hyperparams[\"pred_state\"].keys()\n",
    "# print(agent_type)\n",
    "# print(desired_scene.agent_presence)\n",
    "# print(list(product(AgentType, repeat=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_history_encoder.weight_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_history_encoder.weight_hh_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_history_encoder.bias_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_history_encoder.bias_hh_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_future_encoder.weight_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_future_encoder.weight_hh_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_future_encoder.bias_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_future_encoder.bias_hh_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_future_encoder.weight_ih_l0_reverse', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_future_encoder.weight_hh_l0_reverse', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_future_encoder.bias_ih_l0_reverse', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_future_encoder.bias_hh_l0_reverse', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_future_encoder/initial_h.weight', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_future_encoder/initial_h.bias', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_future_encoder/initial_c.weight', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/node_future_encoder/initial_c.bias', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/edge_influence_encoder.in_proj_weight', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/edge_influence_encoder.in_proj_bias', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/edge_influence_encoder.out_proj.weight', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/edge_influence_encoder.out_proj.bias', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/p_z_x.weight', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/p_z_x.bias', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/hx_to_z.weight', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/hx_to_z.bias', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/hxy_to_z.weight', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/hxy_to_z.bias', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/state_action.weight', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/state_action.bias', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/rnn.weight_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/rnn.weight_hh_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/rnn.bias_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/rnn.bias_hh_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/rnn_cell.weight_ih', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/rnn_cell.weight_hh', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/rnn_cell.bias_ih', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/rnn_cell.bias_hh', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/initial_h.weight', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/initial_h.bias', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/post_rnn.weight', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/post_rnn.bias', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_log_pis.weight', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_log_pis.bias', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_mus.weight', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_mus.bias', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_log_sigmas.weight', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_log_sigmas.bias', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_corrs.weight', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_corrs.bias', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.UNKNOWN->PEDESTRIAN/edge_encoder.weight_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.UNKNOWN->PEDESTRIAN/edge_encoder.weight_hh_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.UNKNOWN->PEDESTRIAN/edge_encoder.bias_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.UNKNOWN->PEDESTRIAN/edge_encoder.bias_hh_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.VEHICLE->PEDESTRIAN/edge_encoder.weight_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.VEHICLE->PEDESTRIAN/edge_encoder.weight_hh_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.VEHICLE->PEDESTRIAN/edge_encoder.bias_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.VEHICLE->PEDESTRIAN/edge_encoder.bias_hh_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN->PEDESTRIAN/edge_encoder.weight_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN->PEDESTRIAN/edge_encoder.weight_hh_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN->PEDESTRIAN/edge_encoder.bias_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.PEDESTRIAN->PEDESTRIAN/edge_encoder.bias_hh_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.BICYCLE->PEDESTRIAN/edge_encoder.weight_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.BICYCLE->PEDESTRIAN/edge_encoder.weight_hh_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.BICYCLE->PEDESTRIAN/edge_encoder.bias_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.BICYCLE->PEDESTRIAN/edge_encoder.bias_hh_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.MOTORCYCLE->PEDESTRIAN/edge_encoder.weight_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.MOTORCYCLE->PEDESTRIAN/edge_encoder.weight_hh_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.MOTORCYCLE->PEDESTRIAN/edge_encoder.bias_ih_l0', 'node_models_dict.PEDESTRIAN.model_registrar.model_dict.MOTORCYCLE->PEDESTRIAN/edge_encoder.bias_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_history_encoder.weight_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_history_encoder.weight_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_history_encoder.bias_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_history_encoder.bias_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_future_encoder.weight_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_future_encoder.weight_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_future_encoder.bias_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_future_encoder.bias_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_future_encoder.weight_ih_l0_reverse', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_future_encoder.weight_hh_l0_reverse', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_future_encoder.bias_ih_l0_reverse', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_future_encoder.bias_hh_l0_reverse', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_future_encoder/initial_h.weight', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_future_encoder/initial_h.bias', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_future_encoder/initial_c.weight', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/node_future_encoder/initial_c.bias', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/edge_influence_encoder.in_proj_weight', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/edge_influence_encoder.in_proj_bias', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/edge_influence_encoder.out_proj.weight', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/edge_influence_encoder.out_proj.bias', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/p_z_x.weight', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/p_z_x.bias', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/hx_to_z.weight', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/hx_to_z.bias', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/hxy_to_z.weight', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/hxy_to_z.bias', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/state_action.weight', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/state_action.bias', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/rnn.weight_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/rnn.weight_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/rnn.bias_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/rnn.bias_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/rnn_cell.weight_ih', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/rnn_cell.weight_hh', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/rnn_cell.bias_ih', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/rnn_cell.bias_hh', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/initial_h.weight', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/initial_h.bias', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/post_rnn.weight', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/post_rnn.bias', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/proj_to_GMM_log_pis.weight', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/proj_to_GMM_log_pis.bias', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/proj_to_GMM_mus.weight', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/proj_to_GMM_mus.bias', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/proj_to_GMM_log_sigmas.weight', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/proj_to_GMM_log_sigmas.bias', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/proj_to_GMM_corrs.weight', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN/decoder/proj_to_GMM_corrs.bias', 'node_models_dict.PEDESTRIAN.node_modules.UNKNOWN->PEDESTRIAN/edge_encoder.weight_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.UNKNOWN->PEDESTRIAN/edge_encoder.weight_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.UNKNOWN->PEDESTRIAN/edge_encoder.bias_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.UNKNOWN->PEDESTRIAN/edge_encoder.bias_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.VEHICLE->PEDESTRIAN/edge_encoder.weight_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.VEHICLE->PEDESTRIAN/edge_encoder.weight_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.VEHICLE->PEDESTRIAN/edge_encoder.bias_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.VEHICLE->PEDESTRIAN/edge_encoder.bias_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN->PEDESTRIAN/edge_encoder.weight_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN->PEDESTRIAN/edge_encoder.weight_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN->PEDESTRIAN/edge_encoder.bias_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.PEDESTRIAN->PEDESTRIAN/edge_encoder.bias_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.BICYCLE->PEDESTRIAN/edge_encoder.weight_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.BICYCLE->PEDESTRIAN/edge_encoder.weight_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.BICYCLE->PEDESTRIAN/edge_encoder.bias_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.BICYCLE->PEDESTRIAN/edge_encoder.bias_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.MOTORCYCLE->PEDESTRIAN/edge_encoder.weight_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.MOTORCYCLE->PEDESTRIAN/edge_encoder.weight_hh_l0', 'node_models_dict.PEDESTRIAN.node_modules.MOTORCYCLE->PEDESTRIAN/edge_encoder.bias_ih_l0', 'node_models_dict.PEDESTRIAN.node_modules.MOTORCYCLE->PEDESTRIAN/edge_encoder.bias_hh_l0', 'model_registrar.model_dict.PEDESTRIAN/node_history_encoder.weight_ih_l0', 'model_registrar.model_dict.PEDESTRIAN/node_history_encoder.weight_hh_l0', 'model_registrar.model_dict.PEDESTRIAN/node_history_encoder.bias_ih_l0', 'model_registrar.model_dict.PEDESTRIAN/node_history_encoder.bias_hh_l0', 'model_registrar.model_dict.PEDESTRIAN/node_future_encoder.weight_ih_l0', 'model_registrar.model_dict.PEDESTRIAN/node_future_encoder.weight_hh_l0', 'model_registrar.model_dict.PEDESTRIAN/node_future_encoder.bias_ih_l0', 'model_registrar.model_dict.PEDESTRIAN/node_future_encoder.bias_hh_l0', 'model_registrar.model_dict.PEDESTRIAN/node_future_encoder.weight_ih_l0_reverse', 'model_registrar.model_dict.PEDESTRIAN/node_future_encoder.weight_hh_l0_reverse', 'model_registrar.model_dict.PEDESTRIAN/node_future_encoder.bias_ih_l0_reverse', 'model_registrar.model_dict.PEDESTRIAN/node_future_encoder.bias_hh_l0_reverse', 'model_registrar.model_dict.PEDESTRIAN/node_future_encoder/initial_h.weight', 'model_registrar.model_dict.PEDESTRIAN/node_future_encoder/initial_h.bias', 'model_registrar.model_dict.PEDESTRIAN/node_future_encoder/initial_c.weight', 'model_registrar.model_dict.PEDESTRIAN/node_future_encoder/initial_c.bias', 'model_registrar.model_dict.PEDESTRIAN/edge_influence_encoder.in_proj_weight', 'model_registrar.model_dict.PEDESTRIAN/edge_influence_encoder.in_proj_bias', 'model_registrar.model_dict.PEDESTRIAN/edge_influence_encoder.out_proj.weight', 'model_registrar.model_dict.PEDESTRIAN/edge_influence_encoder.out_proj.bias', 'model_registrar.model_dict.PEDESTRIAN/p_z_x.weight', 'model_registrar.model_dict.PEDESTRIAN/p_z_x.bias', 'model_registrar.model_dict.PEDESTRIAN/hx_to_z.weight', 'model_registrar.model_dict.PEDESTRIAN/hx_to_z.bias', 'model_registrar.model_dict.PEDESTRIAN/hxy_to_z.weight', 'model_registrar.model_dict.PEDESTRIAN/hxy_to_z.bias', 'model_registrar.model_dict.PEDESTRIAN/decoder/state_action.weight', 'model_registrar.model_dict.PEDESTRIAN/decoder/state_action.bias', 'model_registrar.model_dict.PEDESTRIAN/decoder/rnn.weight_ih_l0', 'model_registrar.model_dict.PEDESTRIAN/decoder/rnn.weight_hh_l0', 'model_registrar.model_dict.PEDESTRIAN/decoder/rnn.bias_ih_l0', 'model_registrar.model_dict.PEDESTRIAN/decoder/rnn.bias_hh_l0', 'model_registrar.model_dict.PEDESTRIAN/decoder/rnn_cell.weight_ih', 'model_registrar.model_dict.PEDESTRIAN/decoder/rnn_cell.weight_hh', 'model_registrar.model_dict.PEDESTRIAN/decoder/rnn_cell.bias_ih', 'model_registrar.model_dict.PEDESTRIAN/decoder/rnn_cell.bias_hh', 'model_registrar.model_dict.PEDESTRIAN/decoder/initial_h.weight', 'model_registrar.model_dict.PEDESTRIAN/decoder/initial_h.bias', 'model_registrar.model_dict.PEDESTRIAN/decoder/post_rnn.weight', 'model_registrar.model_dict.PEDESTRIAN/decoder/post_rnn.bias', 'model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_log_pis.weight', 'model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_log_pis.bias', 'model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_mus.weight', 'model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_mus.bias', 'model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_log_sigmas.weight', 'model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_log_sigmas.bias', 'model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_corrs.weight', 'model_registrar.model_dict.PEDESTRIAN/decoder/proj_to_GMM_corrs.bias', 'model_registrar.model_dict.UNKNOWN->PEDESTRIAN/edge_encoder.weight_ih_l0', 'model_registrar.model_dict.UNKNOWN->PEDESTRIAN/edge_encoder.weight_hh_l0', 'model_registrar.model_dict.UNKNOWN->PEDESTRIAN/edge_encoder.bias_ih_l0', 'model_registrar.model_dict.UNKNOWN->PEDESTRIAN/edge_encoder.bias_hh_l0', 'model_registrar.model_dict.VEHICLE->PEDESTRIAN/edge_encoder.weight_ih_l0', 'model_registrar.model_dict.VEHICLE->PEDESTRIAN/edge_encoder.weight_hh_l0', 'model_registrar.model_dict.VEHICLE->PEDESTRIAN/edge_encoder.bias_ih_l0', 'model_registrar.model_dict.VEHICLE->PEDESTRIAN/edge_encoder.bias_hh_l0', 'model_registrar.model_dict.PEDESTRIAN->PEDESTRIAN/edge_encoder.weight_ih_l0', 'model_registrar.model_dict.PEDESTRIAN->PEDESTRIAN/edge_encoder.weight_hh_l0', 'model_registrar.model_dict.PEDESTRIAN->PEDESTRIAN/edge_encoder.bias_ih_l0', 'model_registrar.model_dict.PEDESTRIAN->PEDESTRIAN/edge_encoder.bias_hh_l0', 'model_registrar.model_dict.BICYCLE->PEDESTRIAN/edge_encoder.weight_ih_l0', 'model_registrar.model_dict.BICYCLE->PEDESTRIAN/edge_encoder.weight_hh_l0', 'model_registrar.model_dict.BICYCLE->PEDESTRIAN/edge_encoder.bias_ih_l0', 'model_registrar.model_dict.BICYCLE->PEDESTRIAN/edge_encoder.bias_hh_l0', 'model_registrar.model_dict.MOTORCYCLE->PEDESTRIAN/edge_encoder.weight_ih_l0', 'model_registrar.model_dict.MOTORCYCLE->PEDESTRIAN/edge_encoder.weight_hh_l0', 'model_registrar.model_dict.MOTORCYCLE->PEDESTRIAN/edge_encoder.bias_ih_l0', 'model_registrar.model_dict.MOTORCYCLE->PEDESTRIAN/edge_encoder.bias_hh_l0'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize online trajectron\n",
    "\n",
    "model_registrar = ModelRegistrar(model_dir, arg_device)\n",
    "\n",
    "trajectron = OnlineTrajectron(model_registrar,\n",
    "                                hyperparams,\n",
    "                                arg_device)\n",
    "\n",
    "agent_types = [AgentType.PEDESTRIAN]\n",
    "edge_types = [(AgentType.PEDESTRIAN, AgentType.PEDESTRIAN)]\n",
    "# print(edge_types)\n",
    "# print(agent_type)\n",
    "# print(edge_types[0])\n",
    "\n",
    "epoch = 5\n",
    "model_path = pathlib.Path(model_dir) / f'model_registrar-{epoch}.pt'\n",
    "checkpoint = torch.load(model_path, map_location=arg_device)\n",
    "trajectron.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectron.set_environment(agent_types, edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 2\n",
      "Agent: 1\n",
      "Agent: 2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[1;32m     41\u001b[0m vis\u001b[38;5;241m.\u001b[39mvisualize_distribution(ax,\n\u001b[1;32m     42\u001b[0m                         dists,\n\u001b[1;32m     43\u001b[0m                         batch_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m{\u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msim_scene\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscene\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximum_history_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m fig\u001b[38;5;241m.\u001b[39msavefig(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_save_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m), dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m     51\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose(fig)\n",
      "File \u001b[0;32m~/Projects/trajectron/adaptive-trajectron-plus-plus/src/trajectron/visualization/visualization.py:106\u001b[0m, in \u001b[0;36mvisualize_prediction\u001b[0;34m(ax, prediction_output_dict, dt, max_hl, ph, robot_node, map, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_prediction\u001b[39m(\n\u001b[1;32m    103\u001b[0m     ax, prediction_output_dict, dt, max_hl, ph, robot_node\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mmap\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    104\u001b[0m ):\n\u001b[0;32m--> 106\u001b[0m     prediction_dict, histories_dict, futures_dict \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_output_to_trajectories\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprediction_output_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_hl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prediction_dict\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prediction_dict\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/trajectron/adaptive-trajectron-plus-plus/src/trajectron/utils/trajectory_utils.py:38\u001b[0m, in \u001b[0;36mprediction_output_to_trajectories\u001b[0;34m(prediction_output_dict, dt, max_h, ph, map, prune_ph_to_future)\u001b[0m\n\u001b[1;32m     35\u001b[0m predictions_output \u001b[38;5;241m=\u001b[39m prediction_output_dict[t][node]\n\u001b[1;32m     36\u001b[0m position_state \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m---> 38\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\n\u001b[1;32m     39\u001b[0m     np\u001b[38;5;241m.\u001b[39marray([t \u001b[38;5;241m-\u001b[39m max_h, t]), position_state\n\u001b[1;32m     40\u001b[0m )  \u001b[38;5;66;03m# History includes current pos\u001b[39;00m\n\u001b[1;32m     41\u001b[0m history \u001b[38;5;241m=\u001b[39m history[\u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misnan(history\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))]\n\u001b[1;32m     43\u001b[0m future \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mget(np\u001b[38;5;241m.\u001b[39marray([t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, t \u001b[38;5;241m+\u001b[39m ph]), position_state)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyperparams[\"maximum_history_length\"] = int((hyperparams[\"history_sec\"]/sim_scene.scene.dt) + 1)\n",
    "for t in range(init_timestep+1, sim_scene.scene.length_timesteps):\n",
    "    print(f\"Timestep: {t}\")\n",
    "\n",
    "    try:\n",
    "        sim_scene: SimulationScene = SimulationScene(\n",
    "        env_name=sim_env_name,\n",
    "        scene_name=scene_name,\n",
    "        scene=desired_scene,\n",
    "        dataset=dataset,\n",
    "        init_timestep=t,\n",
    "        freeze_agents=True,\n",
    "        )\n",
    "        obs: AgentBatch = sim_scene.reset()\n",
    "\n",
    "        start = time.time()\n",
    "        dists, preds = trajectron.incremental_forward(\n",
    "            obs=obs,\n",
    "            maps=None,\n",
    "            prediction_horizon=6,\n",
    "            num_samples=1,\n",
    "            full_dist=True\n",
    "        )\n",
    "        end = time.time()\n",
    "\n",
    "        detailed_preds_dict = dict()\n",
    "        for name in obs.agent_name:\n",
    "            if name in preds:\n",
    "                detailed_preds_dict[name] = preds[name]\n",
    "    except ValueError as e:\n",
    "        if str(e)==f\"Initial timestep {t} contains no agents after filtering. Please choose another initial timestep.\":\n",
    "            obs = None\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # print(\"t=%d: took %.2f s (= %.2f Hz) w/ %d nodes\" % (t, end - start,\n",
    "    #                                                     1. / (end - start), len(obs.agent_name)))\n",
    "                                                        # trajectron.scene_graph.get_num_edges()))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    vis.visualize_distribution(ax,\n",
    "                            dists,\n",
    "                            batch_idx=0)\n",
    "    vis.visualize_prediction(ax,\n",
    "                            {t: preds},\n",
    "                            sim_scene.scene.dt,\n",
    "                            max_h=hyperparams['maximum_history_length'],\n",
    "                            ph=6)\n",
    "    \n",
    "    fig.savefig(os.path.join(output_save_dir, f'pred_{t}.pdf'), dpi=300)\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaptive-trajectron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
